{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77174267",
   "metadata": {},
   "source": [
    "# 01 Data Exploration & Preprocessing\n",
    "\n",
    "Explore and preprocess the SMS Spam Collection Dataset with 5,574 messages.\n",
    "\n",
    "**Dataset**: `hw3_dataset/sms_spam_no_header.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d90de3",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2462254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import preprocessing functions\n",
    "from src.preprocessing import (\n",
    "    load_sms_data,\n",
    "    clean_text,\n",
    "    tokenize_and_stem,\n",
    "    vectorize_text,\n",
    "    preprocess_pipeline\n",
    ")\n",
    "\n",
    "# Set styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37f4c4",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1038c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the formal SMS Spam dataset\n",
    "csv_path = '../hw3_dataset/sms_spam_no_header.csv'\n",
    "\n",
    "texts, labels, label_names = load_sms_data(csv_path)\n",
    "\n",
    "print(f\"âœ… Dataset loaded successfully from: {csv_path}\")\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Total messages: {len(texts):,}\")\n",
    "print(f\"   Label names: {label_names}\")\n",
    "\n",
    "ham_count = sum(1 for l in labels if l == 0)\n",
    "spam_count = sum(1 for l in labels if l == 1)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Label Distribution:\")\n",
    "print(f\"   Ham (Legitimate): {ham_count:,} ({ham_count/len(labels)*100:.1f}%)\")\n",
    "print(f\"   Spam: {spam_count:,} ({spam_count/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7075a",
   "metadata": {},
   "source": [
    "## 3. Explore Sample Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d559d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample HAM messages\n",
    "print(\"Sample HAM messages (first 3):\")\n",
    "print(\"=\" * 100)\n",
    "ham_count_shown = 0\n",
    "for text, label in zip(texts, labels):\n",
    "    if label == 0 and ham_count_shown < 3:\n",
    "        print(f\"Length: {len(text)} chars\")\n",
    "        print(f\"Text: {text}\")\n",
    "        print(\"-\" * 100)\n",
    "        ham_count_shown += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93350bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample SPAM messages\n",
    "print(\"Sample SPAM messages (first 3):\")\n",
    "print(\"=\" * 100)\n",
    "spam_count_shown = 0\n",
    "for text, label in zip(texts, labels):\n",
    "    if label == 1 and spam_count_shown < 3:\n",
    "        print(f\"Length: {len(text)} chars\")\n",
    "        print(f\"Text: {text}\")\n",
    "        print(\"-\" * 100)\n",
    "        spam_count_shown += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d41841",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show text cleaning process\n",
    "sample_texts = [\n",
    "    \"FreeMsg: Hey check this out! Visit http://example.com NOW!!! PRIZE MONEY!!!\",\n",
    "    \"Hello, how are you doing? Email me@example.com for details.\",\n",
    "    \"U dun say so early hor... U c already then say... 123 456!!!\"\n",
    "]\n",
    "\n",
    "print(\"Text Cleaning Pipeline Demonstration:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    cleaned = clean_text(text)\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Original:  {text}\")\n",
    "    print(f\"  Cleaned:   {cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82465070",
   "metadata": {},
   "source": [
    "## 5. Tokenization & Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate tokenization and stemming\n",
    "example_texts = [\n",
    "    \"running runners running quickly run\",\n",
    "    \"connected connecting connection\"\n",
    "]\n",
    "\n",
    "print(\"Tokenization & Stemming Demonstration:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for text in example_texts:\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = tokenize_and_stem(cleaned, remove_stopwords=False)\n",
    "    \n",
    "    print(f\"\\nOriginal:  {text}\")\n",
    "    print(f\"Cleaned:   {cleaned}\")\n",
    "    print(f\"Tokens:    {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d414b",
   "metadata": {},
   "source": [
    "## 6. Message Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze message lengths\n",
    "message_lengths = [len(text) for text in texts]\n",
    "\n",
    "print(f\"Message Length Statistics:\")\n",
    "print(f\"  Min length: {min(message_lengths)} characters\")\n",
    "print(f\"  Max length: {max(message_lengths)} characters\")\n",
    "print(f\"  Mean length: {np.mean(message_lengths):.1f} characters\")\n",
    "print(f\"  Median length: {np.median(message_lengths):.1f} characters\")\n",
    "\n",
    "# Plot distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# All messages\n",
    "axes[0].hist(message_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Message Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Message Lengths')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# By label\n",
    "ham_lengths = [len(text) for text, label in zip(texts, labels) if label == 0]\n",
    "spam_lengths = [len(text) for text, label in zip(texts, labels) if label == 1]\n",
    "\n",
    "axes[1].hist([ham_lengths, spam_lengths], bins=50, label=['Ham', 'Spam'], \n",
    "              color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Message Length (characters)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Message Length by Label')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c008a30",
   "metadata": {},
   "source": [
    "## 7. Complete Preprocessing Pipeline with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d44748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete preprocessing pipeline\n",
    "print(\"Running complete preprocessing pipeline...\")\n",
    "print(\"This may take a few moments...\\n\")\n",
    "\n",
    "result = preprocess_pipeline(csv_path, method='tfidf', max_features=5000, min_df=1, max_df=1.0)\n",
    "\n",
    "X = result['X']\n",
    "y = result['y']\n",
    "feature_names = result['feature_names']\n",
    "metadata = result['metadata']\n",
    "\n",
    "print(f\"âœ… Preprocessing complete!\")\n",
    "print(f\"\\nðŸ“Š Vectorization Statistics:\")\n",
    "print(f\"   Feature matrix shape: {X.shape}\")\n",
    "print(f\"   Number of features: {len(feature_names)}\")\n",
    "print(f\"   Number of non-zero entries: {X.nnz:,}\")\n",
    "print(f\"   Sparsity: {metadata['sparsity']*100:.2f}%\")\n",
    "print(f\"   Method: {metadata['method'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09daad90",
   "metadata": {},
   "source": [
    "## 8. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e72ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top features\n",
    "tfidf_vectorizer = result['vectorizer']\n",
    "feature_importance = np.asarray(X.mean(axis=0)).ravel()\n",
    "\n",
    "# Get top features\n",
    "top_indices = np.argsort(feature_importance)[-20:][::-1]\n",
    "top_features = [feature_names[i] for i in top_indices]\n",
    "top_scores = feature_importance[top_indices]\n",
    "\n",
    "print(\"Top 20 Most Important Features (by average TF-IDF score):\")\n",
    "print(\"=\" * 60)\n",
    "for i, (feature, score) in enumerate(zip(top_features, top_scores), 1):\n",
    "    print(f\"{i:2d}. {feature:20s} Score: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f25979",
   "metadata": {},
   "source": [
    "## 9. Label Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6270769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label distribution visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "labels_pie = ['Ham', 'Spam']\n",
    "sizes = [ham_count, spam_count]\n",
    "colors = ['#2ca02c', '#d62728']\n",
    "explode = (0.05, 0.05)\n",
    "\n",
    "axes[0].pie(sizes, explode=explode, labels=labels_pie, autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90, textprops={'fontsize': 12, 'weight': 'bold'})\n",
    "axes[0].set_title('Label Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "axes[1].bar(labels_pie, sizes, color=colors, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Message Count by Label', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, (label, size) in enumerate(zip(labels_pie, sizes)):\n",
    "    axes[1].text(i, size + 100, f'{size:,}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb757b",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š DATASET INFORMATION\")\n",
    "print(f\"  Dataset File: {csv_path}\")\n",
    "print(f\"  Total Messages: {len(texts):,}\")\n",
    "print(f\"  Ham Messages: {ham_count:,} ({ham_count/len(labels)*100:.1f}%)\")\n",
    "print(f\"  Spam Messages: {spam_count:,} ({spam_count/len(labels)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nðŸ“ MESSAGE LENGTH STATISTICS\")\n",
    "print(f\"  Min: {min(message_lengths)} characters\")\n",
    "print(f\"  Max: {max(message_lengths)} characters\")\n",
    "print(f\"  Mean: {np.mean(message_lengths):.1f} characters\")\n",
    "print(f\"  Median: {np.median(message_lengths):.1f} characters\")\n",
    "\n",
    "print(\"\\nðŸ”¤ PREPROCESSING RESULTS\")\n",
    "print(f\"  Vectorization Method: TF-IDF\")\n",
    "print(f\"  Total Features: {len(feature_names):,}\")\n",
    "print(f\"  Feature Matrix Shape: {X.shape}\")\n",
    "print(f\"  Non-zero Entries: {X.nnz:,}\")\n",
    "print(f\"  Sparsity: {metadata['sparsity']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nâœ… Data exploration complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd6b2b",
   "metadata": {},
   "source": [
    "# SMS Spam Data Exploration and Preprocessing Demo\n",
    "\n",
    "This notebook demonstrates the complete data preprocessing pipeline for SMS spam classification:\n",
    "1. Loading SMS spam data from CSV\n",
    "2. Data exploration and statistics\n",
    "3. Text cleaning and normalization\n",
    "4. Tokenization and stemming\n",
    "5. Bag-of-Words (BoW) vectorization\n",
    "6. TF-IDF vectorization\n",
    "7. Feature analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b357f5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import preprocessing functions\n",
    "from src.preprocessing import (\n",
    "    load_sms_data,\n",
    "    clean_text,\n",
    "    tokenize_and_stem,\n",
    "    vectorize_text,\n",
    "    preprocess_pipeline\n",
    ")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434ad5c",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ec6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "csv_path = '../data/sample_sms_spam.csv'\n",
    "\n",
    "texts, labels, label_names = load_sms_data(csv_path)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Total messages: {len(texts)}\")\n",
    "print(f\"Label names: {label_names}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  Ham: {sum(1 for l in labels if l == 0)}\")\n",
    "print(f\"  Spam: {sum(1 for l in labels if l == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef66295",
   "metadata": {},
   "source": [
    "## 3. Display Sample Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample ham messages\n",
    "print(\"Sample HAM messages (first 2):\")\n",
    "print(\"=\" * 80)\n",
    "for i, (text, label) in enumerate(zip(texts[:5], labels[:5])):\n",
    "    if label == 0:\n",
    "        print(f\"Message {i+1}:\")\n",
    "        print(f\"  Length: {len(text)} characters\")\n",
    "        print(f\"  Text: {text[:100]}...\\n\")\n",
    "\n",
    "print(\"\\nSample SPAM messages (first 2):\")\n",
    "print(\"=\" * 80)\n",
    "for i, (text, label) in enumerate(zip(texts, labels)):\n",
    "    if label == 1:\n",
    "        print(f\"Message {i+1}:\")\n",
    "        print(f\"  Length: {len(text)} characters\")\n",
    "        print(f\"  Text: {text[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f1c87d",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f54e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cleaning process\n",
    "sample_texts = [\n",
    "    \"FreeMsg: Hey check this out! Visit http://example.com NOW!!!\",\n",
    "    \"Hello, how are you? Email me@example.com\",\n",
    "    \"U dun say so early hor... 123 456\"\n",
    "]\n",
    "\n",
    "print(\"Text Cleaning Demonstration:\")\n",
    "print(\"=\" * 80)\n",
    "for text in sample_texts:\n",
    "    cleaned = clean_text(text)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090c8a8",
   "metadata": {},
   "source": [
    "## 5. Tokenization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tokenization and stemming\n",
    "sample_cleaned = \"running runner runs quickly fast\"\n",
    "\n",
    "tokens = tokenize_and_stem(sample_cleaned, remove_stopwords=False)\n",
    "tokens_no_stopwords = tokenize_and_stem(sample_cleaned, remove_stopwords=True)\n",
    "\n",
    "print(\"Tokenization and Stemming:\")\n",
    "print(f\"Original text: {sample_cleaned}\")\n",
    "print(f\"\\nTokens (with stopwords): {tokens}\")\n",
    "print(f\"Tokens (without stopwords): {tokens_no_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b5653",
   "metadata": {},
   "source": [
    "## 6. Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145be223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full preprocessing pipeline\n",
    "result = preprocess_pipeline(csv_path, method='tfidf', max_features=1000)\n",
    "\n",
    "X = result['X']\n",
    "y = result['y']\n",
    "feature_names = result['feature_names']\n",
    "metadata = result['metadata']\n",
    "\n",
    "print(\"Preprocessing Pipeline Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for key, value in metadata.items():\n",
    "    if key == 'label_distribution':\n",
    "        print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f39487",
   "metadata": {},
   "source": [
    "## 7. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most important features\n",
    "feature_importance = np.asarray(X.mean(axis=0)).ravel()\n",
    "top_feature_indices = np.argsort(feature_importance)[-20:]\n",
    "\n",
    "print(\"Top 20 Most Important Features (by TF-IDF):\")\n",
    "print(\"=\" * 80)\n",
    "for idx in reversed(top_feature_indices):\n",
    "    print(f\"{feature_names[idx]:20s} - Score: {feature_importance[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49e6b5",
   "metadata": {},
   "source": [
    "## 8. Visualization: Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar plot\n",
    "labels_count = [sum(y == 0), sum(y == 1)]\n",
    "axes[0].bar(['Ham', 'Spam'], labels_count, color=['green', 'red'], alpha=0.7)\n",
    "axes[0].set_title('Label Distribution')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(labels_count):\n",
    "    axes[0].text(i, v + 0.1, str(v), ha='center')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(labels_count, labels=['Ham', 'Spam'], autopct='%1.1f%%',\n",
    "            colors=['green', 'red'], startangle=90)\n",
    "axes[1].set_title('Label Proportion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Ham: {labels_count[0]} ({100*labels_count[0]/len(y):.1f}%)\")\n",
    "print(f\"Spam: {labels_count[1]} ({100*labels_count[1]/len(y):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b21b0",
   "metadata": {},
   "source": [
    "## 9. Bag-of-Words Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0547297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing with Bag-of-Words\n",
    "result_bow = preprocess_pipeline(csv_path, method='bow', max_features=1000)\n",
    "\n",
    "X_bow = result_bow['X']\n",
    "feature_names_bow = result_bow['feature_names']\n",
    "\n",
    "print(\"Vectorization Method Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"TF-IDF:\")\n",
    "print(f\"  Matrix shape: {X.shape}\")\n",
    "print(f\"  Data type: {X.dtype}\")\n",
    "print(f\"  Sparsity: {(1 - X.nnz / (X.shape[0] * X.shape[1])):.2%}\")\n",
    "print(f\"\\nBag-of-Words:\")\n",
    "print(f\"  Matrix shape: {X_bow.shape}\")\n",
    "print(f\"  Data type: {X_bow.dtype}\")\n",
    "print(f\"  Sparsity: {(1 - X_bow.nnz / (X_bow.shape[0] * X_bow.shape[1])):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b00384",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4295266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the formal SMS Spam dataset\n",
    "csv_path = '../hw3_dataset/sms_spam_no_header.csv'\n",
    "\n",
    "texts, labels, label_names = load_sms_data(csv_path)\n",
    "\n",
    "print(f\"âœ… Dataset loaded successfully from: {csv_path}\")\n",
    "print(f\"ðŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Total messages: {len(texts):,}\")\n",
    "print(f\"   Label names: {label_names}\")\n",
    "print(f\"\\nðŸ“ˆ Label distribution:\")\n",
    "ham_count = sum(1 for l in labels if l == 0)\n",
    "spam_count = sum(1 for l in labels if l == 1)\n",
    "print(f\"   Ham (Legitimate): {ham_count:,} ({ham_count/len(labels)*100:.1f}%)\")\n",
    "print(f\"   Spam: {spam_count:,} ({spam_count/len(labels)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

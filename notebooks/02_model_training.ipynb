{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f43caad",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "This notebook demonstrates the complete model training pipeline:\n",
    "1. Load preprocessed SMS data\n",
    "2. Split into training and testing sets\n",
    "3. Train three classifiers:\n",
    "   - Logistic Regression\n",
    "   - Multinomial Naïve Bayes\n",
    "   - Linear SVM\n",
    "4. Evaluate and compare model performance\n",
    "5. Display metrics and confusion matrices\n",
    "6. Save trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13fde4f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import preprocessing and model functions\n",
    "from src.preprocessing import preprocess_pipeline\n",
    "from src.models import (\n",
    "    train_models,\n",
    "    save_all_models,\n",
    "    compare_models\n",
    ")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379aca11",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "csv_path = '../data/sample_sms_spam.csv'\n",
    "\n",
    "print(\"Running preprocessing pipeline...\")\n",
    "result = preprocess_pipeline(csv_path, method='tfidf', min_df=1)\n",
    "\n",
    "X = result['X']\n",
    "y = result['y']\n",
    "feature_names = result['feature_names']\n",
    "metadata = result['metadata']\n",
    "\n",
    "print(\"\\nPreprocessing Complete!\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Data split: {sum(y == 0)} ham, {sum(y == 1)} spam\")\n",
    "print(f\"Matrix sparsity: {(1 - X.nnz / (X.shape[0] * X.shape[1])):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2fbf07",
   "metadata": {},
   "source": [
    "## 3. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af151b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\"Starting model training...\\n\")\n",
    "training_result = train_models(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = training_result['models']\n",
    "evaluations = training_result['evaluations']\n",
    "data_info = training_result['data_info']\n",
    "\n",
    "print(\"\\nModel Training Complete!\")\n",
    "print(f\"Training samples: {data_info['train_size']}\")\n",
    "print(f\"Test samples: {data_info['test_size']}\")\n",
    "print(f\"Training split:\")\n",
    "print(f\"  - Ham: {data_info['train_ham']}\")\n",
    "print(f\"  - Spam: {data_info['train_spam']}\")\n",
    "print(f\"Test split:\")\n",
    "print(f\"  - Ham: {data_info['test_ham']}\")\n",
    "print(f\"  - Spam: {data_info['test_spam']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9462b",
   "metadata": {},
   "source": [
    "## 4. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "compare_models(evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7006ce",
   "metadata": {},
   "source": [
    "## 5. Detailed Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52208891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed metrics for each model\n",
    "for model_name, eval_dict in evaluations.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name.upper().replace('_', ' ')}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nAccuracy:  {eval_dict['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {eval_dict['precision']:.4f}\")\n",
    "    print(f\"Recall:    {eval_dict['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {eval_dict['f1_score']:.4f}\")\n",
    "    if eval_dict['roc_auc']:\n",
    "        print(f\"ROC-AUC:   {eval_dict['roc_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(eval_dict['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650a85d",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ede05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "model_names = ['Logistic Regression', 'Multinomial Naïve Bayes', 'Linear SVM']\n",
    "model_keys = ['logistic_regression', 'naive_bayes', 'svm']\n",
    "\n",
    "for idx, (ax, model_name, model_key) in enumerate(zip(axes, model_names, model_keys)):\n",
    "    conf_matrix = evaluations[model_key]['confusion_matrix']\n",
    "    \n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'],\n",
    "                cbar=False, ax=ax)\n",
    "    ax.set_title(model_name, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion Matrices:\")\n",
    "print(\"-\" * 80)\n",
    "for model_name, model_key in zip(model_names, model_keys):\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(evaluations[model_key]['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b0826",
   "metadata": {},
   "source": [
    "## 7. Metrics Comparison Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "metrics_data = []\n",
    "for model_key, model_name in zip(model_keys, model_names):\n",
    "    eval_dict = evaluations[model_key]\n",
    "    metrics_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': eval_dict['accuracy'],\n",
    "        'Precision': eval_dict['precision'],\n",
    "        'Recall': eval_dict['recall'],\n",
    "        'F1-Score': eval_dict['f1_score']\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "x = np.arange(len(metrics_df))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - 1.5*width, metrics_df['Accuracy'], width, label='Accuracy')\n",
    "ax.bar(x - 0.5*width, metrics_df['Precision'], width, label='Precision')\n",
    "ax.bar(x + 0.5*width, metrics_df['Recall'], width, label='Recall')\n",
    "ax.bar(x + 1.5*width, metrics_df['F1-Score'], width, label='F1-Score')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_df['Model'])\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMetrics Dataframe:\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afe0ed",
   "metadata": {},
   "source": [
    "## 8. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models\n",
    "print(\"Saving trained models...\")\n",
    "saved_paths = save_all_models(models, models_dir='../models')\n",
    "\n",
    "print(\"\\nModels saved successfully!\")\n",
    "print(\"\\nSaved model paths:\")\n",
    "for model_name, path in saved_paths.items():\n",
    "    print(f\"  {model_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb78cd2",
   "metadata": {},
   "source": [
    "## 9. Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecabdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best performing model\n",
    "print(\"\\nBest Models by Metric:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_accuracy = max([(k, v['accuracy']) for k, v in evaluations.items()], key=lambda x: x[1])\n",
    "best_f1 = max([(k, v['f1_score']) for k, v in evaluations.items()], key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nBest Accuracy: {best_accuracy[0].replace('_', ' ').title()}\")\n",
    "print(f\"  Score: {best_accuracy[1]:.4f}\")\n",
    "\n",
    "print(f\"\\nBest F1-Score: {best_f1[0].replace('_', ' ').title()}\")\n",
    "print(f\"  Score: {best_f1[1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Model training and evaluation complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use best model for predictions\")\n",
    "print(\"2. Fine-tune hyperparameters\")\n",
    "print(\"3. Deploy to Streamlit application\")\n",
    "print(\"4. Monitor model performance in production\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
